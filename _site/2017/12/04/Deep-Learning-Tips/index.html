<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    
    
    <title>Deep Learning Tips</title>
    <meta name="description" content="I often find myself in roughly this situation: I’ve done a lot of work to pre-process data, researched different DL architectures and selected one (or more), modified it so that it works well on my data. And now I’ve got a model that does well, but not quite as well as I want it to. Below I’ll outline steps that I find very useful in this situation. Of course, more detail about how to get to that point is for another post. Also, I’m using Keras so many of these are specific to that tool, but they could absolutely be applied to other DL packages. Clean up training code and improve logging. My typical workflow when I’m building or tweaking a model by hand is to run Kears in a Jupyter notebook. This works fine if the entire pipeline runs in a few minutes but doesn’t work if I need to close my laptop while the pipeline is running. Jupyter often has trouble gracefully reconnecting and then I lose all the verbose info. Porting my code into a self-contained python script allows me to connect to a machine over mosh and tmux, run the script, and then forget about it. This is handy on it’s own but it’ll be extremely handy when we get to the later tips. The tools I use for this are: keras.callbacks.CSVLogger(...) keras.callbacks.TensorBoard(...) keras.callbacks.ModelCheckpoint(..., save_best_only=True) CSVLogger takes the output from verbose and stores it in a csv file for later. If you’re using tmux, it should be preserving your terminal log (and the verbose output) but csv puts all of that history data in a format where you can easily use it. TensorBoard is super handy if you want to be able to monitor the progress of a long-running model from somewhere other than your terminal. I can access this from my iPad and it looks great! ModelCheckpoint saves your model every so often (you set the frequency as a parameter). This is nice on its own because it allows you to access a model that was trained in a script from anywhere (including my preferred tinkering environment, Jupyter). Even more than that, if you use save_best_only=True, then the script is automatically saving space by only saving the best performing model (you should youse val_loss or some other validation metric with this option). Learning Rate Schedule keras.callbacks.LearningRateScheduler keras.callbacks.ReduceLROnPlateau This is my preference. I use this in conjunction with EarlyStopping and ModelCheckpoint all the time. Use noise and other transformations to enlarge your datasets keras.preprocessing.image.ImageDataGenerator() Tensorflow blog discussing this with background noise Optimize There are many hyperparameters that can be optimized: Learning rate Dropout rate Preprocessing steps Different architectures (size and shape of layers as well as count) and parameters Regularizers and parameters Initializers and parameters and more! These create an enormous space of possible models for which to test. My recommendation is to find something that works and then from that functioning model determine plausible options for the above hyperparameters. Then throw what you have into hyperopt. HyperOpt does this automatically. It’s poorly documented but fairly easy to use.">
    

    <!-- Configure mathjax according to https://docs.mathjax.org/en/latest/configuration.html#using-plain-javascript -->
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [
                    ['$', '$'],
                    ["\\(", "\\)"]
                ],
                processEscapes: true
            }
        };
    </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>


    <link rel="stylesheet" href="/assets/soph.css">
    <link rel="stylesheet" href="/assets/main.css">
    <link rel="canonical" href="http://soph.info/2017/12/04/Deep-Learning-Tips/">
    
    
    <link rel="alternate" type="application/rss+xml" title="soph" href="http://soph.info/feed.xml">

    

    
    <meta property="og:type" content="article" />
    <meta property="fb:admins" content="123">
    <meta property="og:title" content="Deep Learning Tips">
    <meta property="og:site_name" content="soph">
    <meta property="og:url" content="http://soph.info/2017/12/04/Deep-Learning-Tips/">
    <meta property="og:description" content="I often find myself in roughly this situation: I’ve done a lot of work to pre-process data, researched different DL architectures and selected one (or more), modified it so that it works well on my data. And now I’ve got a model that does well, but not quite as well as I want it to. Below I’ll outline steps that I find very useful in this situation. Of course, more detail about how to get to that point is for another post. Also, I’m using Keras so many of these are specific to that tool, but they could absolutely be applied to other DL packages. Clean up training code and improve logging. My typical workflow when I’m building or tweaking a model by hand is to run Kears in a Jupyter notebook. This works fine if the entire pipeline runs in a few minutes but doesn’t work if I need to close my laptop while the pipeline is running. Jupyter often has trouble gracefully reconnecting and then I lose all the verbose info. Porting my code into a self-contained python script allows me to connect to a machine over mosh and tmux, run the script, and then forget about it. This is handy on it’s own but it’ll be extremely handy when we get to the later tips. The tools I use for this are: keras.callbacks.CSVLogger(...) keras.callbacks.TensorBoard(...) keras.callbacks.ModelCheckpoint(..., save_best_only=True) CSVLogger takes the output from verbose and stores it in a csv file for later. If you’re using tmux, it should be preserving your terminal log (and the verbose output) but csv puts all of that history data in a format where you can easily use it. TensorBoard is super handy if you want to be able to monitor the progress of a long-running model from somewhere other than your terminal. I can access this from my iPad and it looks great! ModelCheckpoint saves your model every so often (you set the frequency as a parameter). This is nice on its own because it allows you to access a model that was trained in a script from anywhere (including my preferred tinkering environment, Jupyter). Even more than that, if you use save_best_only=True, then the script is automatically saving space by only saving the best performing model (you should youse val_loss or some other validation metric with this option). Learning Rate Schedule keras.callbacks.LearningRateScheduler keras.callbacks.ReduceLROnPlateau This is my preference. I use this in conjunction with EarlyStopping and ModelCheckpoint all the time. Use noise and other transformations to enlarge your datasets keras.preprocessing.image.ImageDataGenerator() Tensorflow blog discussing this with background noise Optimize There are many hyperparameters that can be optimized: Learning rate Dropout rate Preprocessing steps Different architectures (size and shape of layers as well as count) and parameters Regularizers and parameters Initializers and parameters and more! These create an enormous space of possible models for which to test. My recommendation is to find something that works and then from that functioning model determine plausible options for the above hyperparameters. Then throw what you have into hyperopt. HyperOpt does this automatically. It’s poorly documented but fairly easy to use.">
    
    <meta property="article:author" content="https://www.facebook.com/defsophiaray">
    
    <meta property="og:image" content="http://soph.info/images/fb_teach.png">
    
    
    <meta name="twitter:card" content="summary_large_image">
    
    <meta name="twitter:site" content="defsophiaray">
    <meta name="twitter:title" content="Deep Learning Tips">
    <meta name="twitter:description" content="I often find myself in roughly this situation: I’ve done a lot of work to pre-process data, researched different DL architectures and selected one (or more), modified it so that it works well on my...">
    
    <meta name="twitter:creator" content="defsophiaray">
    <meta name="twitter:site" content="defsophiaray">
    
    
    <meta name="twitter:image" content="http://soph.info/images/fb_teach.png">
    

    <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function(d) {
    var wf = d.createElement('script'), s = d.scripts[0];
    wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js';
    wf.async = true;
    s.parentNode.insertBefore(wf, s);
  })(document);
</script>

    
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-103309791-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">soph</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Deep Learning Tips</h1>
    
    <p class="post-meta"><time datetime="2017-12-04T13:15:11+00:00" itemprop="datePublished">Dec 4, 2017</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>I often find myself in roughly this situation:</p>

<blockquote>
  <p>I’ve done a lot of work to pre-process data, researched different DL architectures and selected one (or more), modified it so that it works well on my data.
And now I’ve got a model that does well, but not quite as well as I want it to.</p>
</blockquote>

<p>Below I’ll outline steps that I find very useful in this situation. Of course, more detail about how to get to that point is for another post. Also, I’m using Keras so many of these are specific to that tool, but they could absolutely be applied to other DL packages.</p>

<h2>Clean up training code and improve logging.</h2>

<p>My typical workflow when I’m building or tweaking a model by hand is to run Kears in a Jupyter notebook. This works fine if the entire pipeline runs in a few minutes but doesn’t work if I need to close my laptop while the pipeline is running. Jupyter often has trouble gracefully reconnecting and then I lose all the <code class="highlighter-rouge">verbose</code> info.</p>

<p>Porting my code into a self-contained python script allows me to connect to a machine over mosh and tmux, run the script, and then forget about it. This is handy on it’s own but it’ll be extremely handy when we get to the later tips.</p>

<p>The tools I use for this are:</p>

<ul>
  <li><a href="https://keras.io/callbacks/#csvlogger"><code class="highlighter-rouge">keras.callbacks.CSVLogger(...)</code></a></li>
  <li><a href="https://keras.io/callbacks/#tensorboard"><code class="highlighter-rouge">keras.callbacks.TensorBoard(...)</code></a></li>
  <li><a href="https://keras.io/callbacks/#modelcheckpoint"><code class="highlighter-rouge">keras.callbacks.ModelCheckpoint(..., save_best_only=True)</code></a></li>
</ul>

<p><code class="highlighter-rouge">CSVLogger</code> takes the output from verbose and stores it in a csv file for later. If you’re using tmux, it should be preserving your terminal log (and the <code class="highlighter-rouge">verbose</code> output) but csv puts all of that history data in a format where you can easily use it.</p>

<p><code class="highlighter-rouge">TensorBoard</code> is super handy if you want to be able to monitor the progress of a long-running model from somewhere other than your terminal. I can access this from my iPad and it looks great!
<img src="/images/tips/tb.png" alt="" /></p>

<p><code class="highlighter-rouge">ModelCheckpoint</code> saves your model every so often (you set the frequency as a parameter). This is nice on its own because it allows you to access a model that was trained in a script from anywhere (including my preferred tinkering environment, Jupyter). Even more than that, if you use <code class="highlighter-rouge">save_best_only=True</code>, then the script is automatically saving space by only saving the best performing model (you should youse <code class="highlighter-rouge">val_loss</code> or some other validation metric with this option).</p>

<h2>Learning Rate Schedule</h2>

<ul>
  <li><a href="https://keras.io/callbacks/#learningratescheduler"><code class="highlighter-rouge">keras.callbacks.LearningRateScheduler</code></a></li>
  <li><a href="https://keras.io/callbacks/#reducelronplateau"><code class="highlighter-rouge">keras.callbacks.ReduceLROnPlateau</code></a> This is my preference. I use this in conjunction with EarlyStopping and ModelCheckpoint all the time.</li>
</ul>

<h2>Use noise and other transformations to enlarge your datasets</h2>

<ul>
  <li><a href="https://keras.io/preprocessing/image/#imagedatagenerator">keras.preprocessing.image.ImageDataGenerator()</a></li>
  <li><a href="https://www.tensorflow.org/versions/master/tutorials/audio_recognition#background_noise">Tensorflow blog discussing this with background noise</a></li>
</ul>

<h2>Optimize</h2>

<p>There are many hyperparameters that can be optimized:</p>
<ul>
  <li>Learning rate</li>
  <li>Dropout rate</li>
  <li>Preprocessing steps</li>
  <li>Different architectures (size and shape of layers as well as count) and parameters</li>
  <li>Regularizers and parameters</li>
  <li>Initializers and parameters</li>
  <li>and more!</li>
</ul>

<p>These create an enormous space of possible models for which to test. My recommendation is to find something that works and then from that functioning model determine plausible options for the above hyperparameters. Then throw what you have into hyperopt.</p>

<ul>
  <li><a href="https://github.com/hyperopt/hyperopt">HyperOpt does this automatically</a>. It’s poorly documented but fairly easy to use.</li>
</ul>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Sophia Ray Searcy - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://soph.info/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
