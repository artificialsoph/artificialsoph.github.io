<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

    
  <title>Deep learning from scratch with python</title>
  <meta name="description" content="Last week I presented at the Data Science Study Group on a project of mine where I built a deep learning platform from scratch in python. For reference, here’s my code and slides. First, my project drew primarily from two sets of sources, without which I never would have completed this project. First, there are many examples of folks doing this online. Here’s an incomplete list in python: ** Deep Learning From Scratch I-V by Daniel Sabinasz ** Implementing a Neural Network from Scratch in Python – An Introduction by Denny Britz Understanding and coding Neural Networks From Scratch in Python and R by Sunil Ray How to Implement the Backpropagation Algorithm From Scratch In Python While all of these are useful, Sabinasz’s was what I based my project on because he implements a system that builds a computational graph and includes a true backpropogation algorithm. The others I saw do this implicitly by calculating the gradients operation-by-operation. That approach is fine for a single demo but I wanted something that mimicked the flexibility of tensorflow, allowing me to compare different network structures and activations without starting over each time. In addition to these resources, I drew heavily from Deep Learning by Goodfellow, Bengio, and Courville. I’m certain that the other examples I looked toward used this book as well. While I started with Sabinasz’s code, I made a few modifications and improvements including: Add graph visualization with python Graphviz Remove the use of globals for the computational graph Simplify backprop algorithm by adding gradient calculations to the operation classes Add a Relu activation function Tweak the visualizations Here’s the learning rate plotted along with the classification boundary for a relu network with 4 hidden nodes. And here’s the computational graph. You can really see the benefit of tracking the graph and automating the backprop algorithm for a graph of this size. What I still want to do I want to write up a blog summarizing my talk and the process for creating this. I think it could be a very useful explanatory tool. I have a strong feeling that some of the gradients in here are inaccurate. In many cases the network fails to learn for any learning rate schedule unless I give it a much higher capacity than it needs (e.g. 4+ hidden nodes in the XOR task). In simple cases, like separable data, the model should be able to get arbitrarily close to $J=0$ but fails to do so. The softmax gradient seems to differ from that found in other sources I want to extend this model to larger datasets and deeper networks. Right now it runs into what I think are underflow errors in these cases but they should be possible to avoid." />
  

  <!-- Configure mathjax according to https://docs.mathjax.org/en/latest/configuration.html#using-plain-javascript -->
  <script type="text/javascript">
    window.MathJax = {
      tex2jax: {
        inlineMath: [["$", "$"], ["\\(", "\\)"]],
        processEscapes: true
      }
    };
  </script>
  <script
    type="text/javascript"
    async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
  ></script>

  <link rel="stylesheet" href="/assets/soph.css"> <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2017/10/18/scratch/" />
   
  <link
    rel="alternate"
    type="application/rss+xml"
    title="soph"
    href="http://localhost:4000/feed.xml"
  />

   
  <meta property="og:type" content="article" />
  <meta property="fb:admins" content="123" />
  <meta property="og:title" content="Deep learning from scratch with python" />
  <meta property="og:site_name" content="soph" />
  <meta property="og:url" content="http://localhost:4000/2017/10/18/scratch/" />
  <meta property="og:description" content="Last week I presented at the Data Science Study Group on a project of mine where I built a deep learning platform from scratch in python. For reference, here’s my code and slides. First, my project drew primarily from two sets of sources, without which I never would have completed this project. First, there are many examples of folks doing this online. Here’s an incomplete list in python: ** Deep Learning From Scratch I-V by Daniel Sabinasz ** Implementing a Neural Network from Scratch in Python – An Introduction by Denny Britz Understanding and coding Neural Networks From Scratch in Python and R by Sunil Ray How to Implement the Backpropagation Algorithm From Scratch In Python While all of these are useful, Sabinasz’s was what I based my project on because he implements a system that builds a computational graph and includes a true backpropogation algorithm. The others I saw do this implicitly by calculating the gradients operation-by-operation. That approach is fine for a single demo but I wanted something that mimicked the flexibility of tensorflow, allowing me to compare different network structures and activations without starting over each time. In addition to these resources, I drew heavily from Deep Learning by Goodfellow, Bengio, and Courville. I’m certain that the other examples I looked toward used this book as well. While I started with Sabinasz’s code, I made a few modifications and improvements including: Add graph visualization with python Graphviz Remove the use of globals for the computational graph Simplify backprop algorithm by adding gradient calculations to the operation classes Add a Relu activation function Tweak the visualizations Here’s the learning rate plotted along with the classification boundary for a relu network with 4 hidden nodes. And here’s the computational graph. You can really see the benefit of tracking the graph and automating the backprop algorithm for a graph of this size. What I still want to do I want to write up a blog summarizing my talk and the process for creating this. I think it could be a very useful explanatory tool. I have a strong feeling that some of the gradients in here are inaccurate. In many cases the network fails to learn for any learning rate schedule unless I give it a much higher capacity than it needs (e.g. 4+ hidden nodes in the XOR task). In simple cases, like separable data, the model should be able to get arbitrarily close to $J=0$ but fails to do so. The softmax gradient seems to differ from that found in other sources I want to extend this model to larger datasets and deeper networks. Right now it runs into what I think are underflow errors in these cases but they should be possible to avoid." />
  
  <meta property="article:author" content="https://www.facebook.com/defsophiaray" />
   
  <meta property="og:image" content="http://soph.info/images/fb_teach.png" />
    <meta name="twitter:card" content="summary_large_image"> 
  <meta name="twitter:site" content="defsophiaray" />
  
  <meta name="twitter:title" content="Deep learning from scratch with python" />
  <meta
    name="twitter:description"
    content="Last week I presented at the Data Science Study Group on a project of mine where I built a deep learning platform from scratch in python. For reference, here’s my code and slides. First, my project..."
  />
  
  <meta name="twitter:creator" content="defsophiaray" />
  <meta name="twitter:site" content="defsophiaray" />
   
  <meta name="twitter:image" content="http://soph.info/images/fb_teach.png" />
   <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function(d) {
    var wf = d.createElement('script'), s = d.scripts[0];
    wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js';
    wf.async = true;
    s.parentNode.insertBefore(wf, s);
  })(document);
</script>
 
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-103309791-1', 'auto');
    ga('send', 'pageview');

  </script>


</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">soph</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Deep learning from scratch with python</h1>
    
    <p class="post-meta"><time datetime="2017-10-18T10:09:10+00:00" itemprop="datePublished">Oct 18, 2017</time>

 •
  
    
    
      
        <a href="/tags/deep/">Deep</a>,
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
        <a href="/tags/learning/">learning,</a>,
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
    
      
        <a href="/tags/data/">Data</a>,
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
    
      
    
      
        <a href="/tags/science/">Science,</a>,
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  
    
    
      
    
      
    
      
    
      
    
      
        <a href="/tags/python/">Python</a>
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Last week I presented at the <a href="https://web.archive.org/web/20171018141234/https://www.meetup.com/NYC-Data-Science-Study-Group/events/243747107/">Data Science Study Group</a> on a project of mine where I built a deep learning platform from scratch in python.</p>

<p>For reference, here’s my <a href="https://github.com/sophiaray/sophscratch">code</a> and <a href="http://soph.info/scratch.pdf">slides</a>.</p>

<p>First, my project drew primarily from two sets of sources, without which I never would have completed this project. First, there are many examples of folks doing this online. Here’s an incomplete list in python:</p>

<ul>
  <li>** <a href="http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/">Deep Learning From Scratch I-V</a> by <a href="https://twitter.com/deepideas_net">Daniel Sabinasz</a> **</li>
  <li><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">Implementing a Neural Network from Scratch in Python – An Introduction</a> by <a href="https://twitter.com/dennybritz">Denny Britz</a></li>
  <li><a href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/">Understanding and coding Neural Networks From Scratch in Python and R</a> by <a href="https://twitter.com/sunil2ray?lang=en">Sunil Ray</a></li>
  <li><a href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/">How to Implement the Backpropagation Algorithm From Scratch In Python</a></li>
</ul>

<p>While all of these are useful, Sabinasz’s was what I based my project on because he implements a system that builds a computational graph and includes a true backpropogation algorithm. The others I saw do this implicitly by calculating the gradients operation-by-operation. That approach is fine for a single demo but I wanted something that mimicked the flexibility of tensorflow, allowing me to compare different network structures and activations without starting over each time.</p>

<p>In addition to these resources, I drew heavily from <a href="http://www.deeplearningbook.org/">Deep Learning</a> by Goodfellow, Bengio, and Courville. I’m certain that the other examples I looked toward used this book as well.</p>

<p>While I started with Sabinasz’s code, I made a few modifications and improvements including:</p>

<ul>
  <li>Add graph visualization with <a href="http://graphviz.readthedocs.io/en/stable/manual.html">python Graphviz</a></li>
  <li>Remove the use of globals for the computational graph</li>
  <li>Simplify backprop algorithm by adding gradient calculations to the operation classes</li>
  <li>Add a Relu activation function</li>
  <li>Tweak the visualizations</li>
</ul>

<p>Here’s the learning rate plotted along with the classification boundary for a relu network with 4 hidden nodes.
<img src="/images/relu-w-hidden-nodes.png" alt="" /></p>

<p>And here’s the computational graph. You can really see the benefit of tracking the graph and automating the backprop algorithm for a graph of this size.
<img src="/images/relu-graph.png" alt="" /></p>

<h2>What I still want to do</h2>

<ol>
  <li>I want to write up a blog summarizing my talk and the process for creating this. I think it could be a very useful explanatory tool.</li>
  <li>I have a strong feeling that some of the gradients in here are inaccurate.
    <ul>
      <li>In many cases the network fails to learn for any learning rate schedule unless I give it a much higher capacity than it needs (e.g. 4+ hidden nodes in the XOR task).</li>
      <li>In simple cases, like separable data, the model should be able to get arbitrarily close to $J=0$ but fails to do so.</li>
      <li>The softmax gradient seems to differ from that found in other sources</li>
    </ul>
  </li>
  <li>I want to extend this model to larger datasets and deeper networks. Right now it runs into what I think are underflow errors in these cases but they should be possible to avoid.</li>
</ol>

  </div>

  

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; Sophia Ray Searcy - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
