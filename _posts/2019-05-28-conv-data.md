---
title: "Slash dataset: A toy dataset that stymies most non-convolutional models."
layout: post
date: 2019-5-28 17:38
permalink: slash-data
tags: ["data science", "howto"]
---

# TLDR

I'm releasing a dataset "Slash dataset" that is easy for convolutional neural nets to learn but difficult for most other models.

It will live here: [soph.info/data/slash_data.npz](https://soph.info/data/slash_data.npz)

Code for generating the dataset and replicating the analysis is at the bottom of this post. Cheers!

## Background

The convolution is an extremely important mathematical operation. It's frequently used in [digital signal processing](https://en.wikipedia.org/wiki/Convolution#Applications), including compression techniques for images and audio, and it's the namesake and heart of ["Convolutional Neural Network"](https://www.deeplearningbook.org/contents/convnets.html).

But what actually _is_ a convolution? There are a bunch of mesmerizing gifs like the one below, and there are some good explanations out there, but I think there's room to improve.
![](https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif)

I'm in the middle of a project to explain exactly what convolution does and, specifically, why it's so useful for machine learning. As part of that effort, I've put together a toy dataset designed to be as simple as possible while also being difficult for all models except for convolutional ones.

## Data

The dataset includes 1,000 images, each of which include a slash. It is divided into two classes according to whether the slash is downward facing (like the backslash character "") or upward facing (like the forward slash ). Below is a random selection along with class labels.

![](/images/conv_data.png)

Each image is 30x30 grayscale pixels. I've added a little bit of noise to the images to make things interesting.

## Results

Hopefully by now, you're looking at this dataset and thinking "gee, that doesn't look _too_ difficult". This would be a boring task for any human and it might be difficult at first to see why it is so challenging for most models.

I'm going to save a detailed explanation of exactly why that is for later. For now, I want to share my results: I tested out several off-the-shelf models as well as two convolutional models.

Of the off-the-shelf models, I didn't waste my time tuning most of them, as I feel quite confident that even when tuned they would do poorly. (I'd love to be proved wrong on that ðŸ˜‰.) The one that I did tune was K-Nearest Neighbors, so you'll see a result for one with the default parameters as well as one with the best performing parameters ($k=1$).

All of the models perform at or close enough to chance (50%) to dismiss them. KNN does pretty well when we tune it (90% test accuracy) but this performance looks less impressive when we note that KNN is memorizing the entire dataset and then comparing new examples to memorized ones. That strategy is not one that we would expect to hold up in the real world where data is much bigger than a few hundred B&W thumbnail images. Because of this, I've listed the trainable parameters for each model.

On the other hand, the ConvNets both get perfect scores on both train and test sets. The **Vanilla CNN** uses the same cookie-cutter structure that I would apply as a first attempt on just about _any_ image data (two rounds of Convolution â†’Batch Normalizationâ†’Pooling). This Vanilla CNN is not tuned to the particular dataset. The **Custom CNN** is one I built and hand-tuned to demonstrate how ConvNets can really punch above their weight in terms of parameters. This architecture was pretty fragile (tweaking the numbers slightly is likely to break it) when testing but it's mostly there to make a point.

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>2</th>
      <th>3</th>
      <th>0</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Model</th>
      <td>Random Forest</td>
      <td>SVM - Linear</td>
      <td>SVM - RBF kernel</td>
      <td>SVM - Polynomial kernel</td>
      <td>Logistic Regression</td>
      <td>K Nearest Neighbors</td>
      <td>KNN, k=1</td>
      <td>Vanilla CNN</td>
      <td>Custom CNN</td>
    </tr>
    <tr>
      <th>Trainable params</th>
      <td>9183</td>
      <td>498600</td>
      <td>675000</td>
      <td>675000</td>
      <td>901</td>
      <td>675000</td>
      <td>675000</td>
      <td>1041</td>
      <td>80</td>
    </tr>
    <tr>
      <th>Train score</th>
      <td>1</td>
      <td>0.994667</td>
      <td>0.838667</td>
      <td>0.84</td>
      <td>0.98</td>
      <td>0.842667</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Test score</th>
      <td>0.472</td>
      <td>0.5</td>
      <td>0.532</td>
      <td>0.532</td>
      <td>0.54</td>
      <td>0.604</td>
      <td>0.908</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

## Methods

All code used to generate this dataset is included below. Be on the lookout for more Convolution content shortly!

<script src="https://gist.github.com/artificialsoph/b71c1c25b5ea86cb7ad3ab38afcbfb55.js"></script>
